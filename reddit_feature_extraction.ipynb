{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-63395686d989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textacy'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "Authors: Daniel M. Low\n",
    "License: Apache 2.0\n",
    "'''\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stanza\n",
    "import textacy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this once at the beginning so we don't reload for each post\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')# initialize English neural pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: Mayor\tner: O\n",
      "token: Cuomo\tner: S-PERSON\n",
      "token: may\tner: O\n",
      "token: lockdown\tner: O\n",
      "token: NYC\tner: S-GPE\n",
      "token: .\tner: O\n",
      "token: I\tner: O\n",
      "token: live\tner: O\n",
      "token: in\tner: O\n",
      "token: Brooklyn\tner: S-GPE\n",
      "token: .\tner: O\n",
      "[{\n",
      "  \"text\": \"Cuomo\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 6,\n",
      "  \"end_char\": 11\n",
      "}, {\n",
      "  \"text\": \"NYC\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 25,\n",
      "  \"end_char\": 28\n",
      "}, {\n",
      "  \"text\": \"Brooklyn\",\n",
      "  \"type\": \"GPE\",\n",
      "  \"start_char\": 40,\n",
      "  \"end_char\": 48\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "post = \"Mayor Cuomo may lockdown NYC. I live in Brooklyn.\"\n",
    "post = nlp(post)\n",
    "print(*[f'token: {token.text}\\tner: {token.ner}' for sent in post.sentences for token in sent.tokens], sep='\\n')\n",
    "print(post.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract location or other entities\n",
    "def extract_entity(doc, entity = 'GPE'):   \n",
    "    # classes although they have different code names like GPE and ORG: (PERSON, LOCATION, ORGANIZATION, MISC), numerical (MONEY, NUMBER, ORDINAL, PERCENT), and temporal (DATE, TIME, DURATION, SET) entities (12 classes).\n",
    "    # Adding the regexner annotator and using the supplied RegexNER pattern files adds support for the fine-grained and additional entity classes EMAIL, URL, CITY, STATE_OR_PROVINCE, COUNTRY, NATIONALITY, RELIGION, (job) TITLE, IDEOLOGY, CRIMINAL_CHARGE, CAUSE_OF_DEATH, HANDLE (12 classes) for a total of 24 classes.\n",
    "    \n",
    "    # Extract locations (don't lowercase doc, won't work for \"nyc\", only \"NYC\")\n",
    "    result = []\n",
    "    doc = nlp(doc) #nlp was loaded once above\n",
    "    entitities = doc.ents\n",
    "\n",
    "    if entity == 'all':\n",
    "        result = [n.text for n in entitities]\n",
    "        return result\n",
    "    else:\n",
    "        for ent in entitities:\n",
    "            if ent.type ==entity:\n",
    "                # append 'NYC'                 \n",
    "                result.append(ent.text)\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brooklyn', 'NYC']\n",
      "['Cuomo']\n",
      "['Brooklyn', 'Cuomo', 'April 21st, 1990']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(extract_entity(\"I live in Brooklyn, NYC\", entity = 'GPE'))\n",
    "print(extract_entity(\"I live in Brooklyn, Cuomo's town\", entity = 'PERSON'))\n",
    "print(extract_entity(\"I live in Brooklyn, Cuomo's town, on April 21st, 1990\", entity = 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(doc,words=[]):\n",
    "    '''\n",
    "    words = ['corona','virus','coronavirus', 'pandemic', 'epidemic', 'quarantine', 'covid', 'covid19']\n",
    "    '''\n",
    "    text_lower = doc.lower()\n",
    "    # remove punctuation except apostrophes because we need to search for things like don't want to live\n",
    "    text_wo_punctuation = re.sub(\"[^\\w\\d'\\s]+\",'',sentence).replace('  ',' ')\n",
    "    counter = 0\n",
    "    for word in words:\n",
    "        counter+= text_wo_punctuation.split().count(word)\n",
    "\n",
    "    if counter>0:\n",
    "        appears = 1\n",
    "    else:\n",
    "        appears = 0\n",
    "    return counter, appears\n",
    "\n",
    "def punctuation_count(doc):\n",
    "    d = {}\n",
    "    punctuation = '!\"#$%&()*+,-./:;=?@[\\]^`{|}~' #removed ', <>, _ because m_c_a_t doesnt reflect punctuation really\n",
    "    for c in document:\n",
    "        if c in punctuation:\n",
    "            if c not in d:\n",
    "                d[c] = 0\n",
    "            d[c] += 1\n",
    "    total = np.sum(list(d.values()))\n",
    "    return total\n",
    "\n",
    "def liwc(input_path, document=None):\n",
    "    liwc = np.load(input_path + 'liwc.npy').item()\n",
    "    categories = pd.read_csv(input_path + 'categories.txt', index_col=0)['0'].tolist()\n",
    "    liwc_vector = []\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    for category in categories:\n",
    "        counter  = 0\n",
    "        document_tokenized = [n.strip(string.punctuation).lower() for n in document.split()]\n",
    "        document_stemmed = [stemmer.stem(word) for word in document_tokenized]\n",
    "        # for each word in category, check if its in stemmed sentence list\n",
    "        counter_doc = np.sum([sum(word.rstrip()==s for s in document_stemmed) for word in liwc.get(category)])# Make sure to remove final space from word with rstrip\n",
    "        '''\n",
    "        # test\n",
    "        category = categories[0]\n",
    "        for word in liwc.get(category):\n",
    "            if word in document_stemmed:\n",
    "                print(word)\n",
    "        '''\n",
    "        counter += counter_doc\n",
    "        liwc_vector.append(counter)\n",
    "    names = ['liwc_'+n for n in categories]\n",
    "    return liwc_vector, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(X_train_sentences = [], X_test_sentences=[], lower_case = True, ngram_range = (1,2), max_features=512, min_df=2, max_df=0.8, model = 'vector'):\n",
    "    \"\"\"\n",
    "    TfidfVectorizer is CountVectorizer followed by TfidfTransformer, The former converts text documents to a sparse matrix of token counts.\n",
    "    This sparse matrix is then put through the TfidfTransformer which converts a count matrix to a normalized Term Frequency-Inverse Document Frquency(tf)  \n",
    "    representation which is a metric of word importance. \n",
    "    We fit_transform on train_sentences and transform on test sentences to prevent overfitting, X_test_sentences can be None\n",
    "    \n",
    "    model: {vector, sequential} depending on what model takes as input: vector (svm, random forest), sequential (lstm)\n",
    "    \"\"\"\n",
    "    sw = stopwords.words('english')\n",
    "\n",
    "    if model == 'sequential':\n",
    "        #         \n",
    "        vectorizer = TfidfVectorizer(lowercase=lower_case, ngram_range=ngram_range, stop_words=sw,\n",
    "                                     max_features=max_features, min_df=min_df, max_df=max_df, analyzer=lambda x: x)\n",
    "        train_vectors = vectorizer.fit_transform(X_train_sentences).toarray()\n",
    "        if X_test_sentences:\n",
    "            test_vectors = vectorizer.transform(X_test_sentences).toarray()\n",
    "\n",
    "    else:\n",
    "        # model = 'vector'         \n",
    "        vectorizer = TfidfVectorizer(lowercase=lower_case, ngram_range=ngram_range, stop_words=sw,\n",
    "                                     max_features=max_features, min_df=min_df, max_df=max_df)\n",
    "        train_vectors = vectorizer.fit_transform(X_train_sentences).toarray()\n",
    "        if X_test_sentences:\n",
    "            test_vectors = vectorizer.transform(X_test_sentences).toarray()\n",
    "    # train_vectors = vectorizer.fit_transform(X_train_sentences.ravel()).toarray()\n",
    "    # test_vectors = vectorizer.transform(X_test_sentences.ravel()).toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    feature_names = ['tfidf_'+n for n in feature_names]\n",
    "    if X_test_sentences:\n",
    "        return train_vectors, test_vectors, feature_names\n",
    "    else:\n",
    "        return train_vectors, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_NLP_features(doc, features):\n",
    "    feature_vector = []\n",
    "    feature_names  = []\n",
    "    \n",
    "    if 'sentiment' in features:\n",
    "        # don't lowercase or remove punctuation, but maybe preprocess emojis     \n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        scores = sid.polarity_scores(doc)\n",
    "        names = ['sent_neg','sent_neu', 'sent_pos', 'sent_compound']\n",
    "        assert len(scores) == len(names)\n",
    "        feature_vector.append(scores)\n",
    "        feature_names.append(names)\n",
    "        \n",
    "    if 'covid19' in features:\n",
    "        # todo: see if we should add more by looking through COVID19_Support to see how people are mentioning it         \n",
    "        words = ['corona','virus','coronavirus', 'pandemic', 'epidemic', 'quarantine', 'covid', 'covid19']\n",
    "        counter, appears = count_words(doc,words=words)\n",
    "        feature_vector.append([counter, appears])\n",
    "        feature_names.append(['covid19_total', 'covid19_boolean'])\n",
    "        \n",
    "    if 'suicidality' in features:\n",
    "        # todo: should we add any more? A suicide researcher gave me some more. We should be good, but we could go through SuicideWatch\n",
    "        words = ['commit suicide', 'jump off a bridge', 'I want to overdose', 'I will overdose', 'thinking about overdose', 'kill myself', 'killing myself', 'hang myself', 'hanging myself', 'cut myself', 'cutting myself', 'hurt myself', 'hurting myself', 'want to diewanna die', \"don't want to wake up\", \"don't wake up\", 'never want to wake up', \"don't want to be alive\", 'want to be alive anymore', 'wish it would all end', 'done with living', 'want it to end', 'it all ends tonight', 'end my life', 'live anymore', 'living anymore', 'life anymore', 'be dead', 'take it anymore', 'think about death', 'hopeless', 'hurt myself', \"no one will miss medon't want to wake up\", 'if I live or die', 'i hate my life', 'shoot me', 'kill me']\n",
    "        counter, appears = count_words(doc,words=words)\n",
    "        feature_vector.append([counter, appears])\n",
    "        feature_names.append(['suicidality_total', 'suicidality_boolean'])\n",
    "        \n",
    "    \n",
    "    if 'punctuation' in features:\n",
    "        count = punctuation_count(doc)\n",
    "        feature_vector.append([count])\n",
    "        feature_names.append(['punctuation'])\n",
    "        \n",
    "    if 'liwc' in features:\n",
    "        input_path = './../data/input/liwc_english_dictionary/'\n",
    "        vector, names = liwc(input_path = input_path, document = doc)\n",
    "        feature_vector.append(vector)\n",
    "        feature_names.append(names)\n",
    "        \n",
    "\n",
    "\n",
    "    if 'basic_count' in features:\n",
    "        # https://chartbeat-labs.github.io/textacy/build/html/getting_started/quickstart.html\n",
    "        '''\n",
    "        {'n_sents': 3,\n",
    "         'n_words': 73,\n",
    "         'n_chars': 414,\n",
    "         'n_syllables': 134,\n",
    "         'n_unique_words': 57,\n",
    "         'n_long_words': 30,\n",
    "         'n_monosyllable_words': 38,\n",
    "         'n_polysyllable_words': 19}\n",
    "        '''\n",
    "        ts = textacy.TextStats(doc)\n",
    "        '''\n",
    "        todo:\n",
    "        scores = ts.basic_counts.values() #or something like this\n",
    "        names = ts.basic_counts.keys() #or something like this\n",
    "        '''\n",
    "        feature_vector.append(scores)\n",
    "        feature_names.append(names)\n",
    "\n",
    "        \n",
    "    if 'readability' in features:\n",
    "        '''\n",
    "        {'flesch_kincaid_grade_level': 15.56027397260274,\n",
    "        'flesch_reading_ease': 26.84351598173518,\n",
    "        'smog_index': 17.5058628484301,\n",
    "        'gunning_fog_index': 20.144292237442922,\n",
    "        'coleman_liau_index': 16.32928468493151,\n",
    "        'automated_readability_index': 17.448173515981736,\n",
    "        'lix': 65.42922374429223,\n",
    "        'gulpease_index': 44.61643835616438,\n",
    "        'wiener_sachtextformel': 11.857779908675797}\n",
    "        '''\n",
    "        ts = textacy.TextStats(doc)\n",
    "        ts.readability_stats\n",
    "        '''\n",
    "        todo:\n",
    "        scores = ts.readability_stats.values() #or something like this\n",
    "        names = ts.readability_stats.keys() #or something like this\n",
    "        '''\n",
    "        feature_vector.append(scores)\n",
    "        feature_names.append(names)\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    feature_vector = [n for i in feature_vector for n in i]\n",
    "    feature_names  = [n for i in feature_names for n in i]\n",
    "    return feature_vector, feature_names\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = ['sentiment', 'covid19', 'suicidality', 'punctuation', 'liwc', 'basic_count', 'readability',  'tfidf']\n",
    "\n",
    "input_dir = './../data/input/'\n",
    "files  = os.listdir(input_dir)\n",
    "for file in files:\n",
    "    df_subreddit = pd.read_csv(input_dir + file)\n",
    "    posts = list(df_subreddit.posts)\n",
    "    posts = [post.replace('\\n', ' ').replace('  ',' ').replace('“', '').replace('”', '') for post in posts] #here I remove paragraph split, double spaces and some other weird stuff, this should be done once for all posts\n",
    "    \n",
    "    for post in posts:\n",
    "        feature_vector, feature_names = extract_NLP_features(post, features)\n",
    "        '''\n",
    "        #     or you can do a list comprehension [extract_NLP_features(post, features) for post in posts]\n",
    "        # the way i have it above, you extract feature names each time, this could be done once. \n",
    "        todo: append to a df_subreddit_features         \n",
    "        '''\n",
    "\n",
    "    if 'tfidf' in features:\n",
    "        \n",
    "    '''\n",
    "    tfidf is done with the whole corpus, not for each sentence, or with the whole train set (if we're doing a split, which we may not be for cluster analysis). \n",
    "    \n",
    "    play with the params a bit to figure out how many max_features so that it's not too sparse and captures important words per subreddit. \n",
    "    '''\n",
    "    tfidf(X_train_sentences = [], X_test_sentences=[], lower_case = True, ngram_range = (1,2), max_features=512, min_df=2, max_df=0.8, model = 'svm')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
