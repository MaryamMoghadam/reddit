{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-Z0DMkxpGsF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def str2boolean(s):\n",
    "\tif s == 'False':\n",
    "\t\ts_new = False\n",
    "\telse:\n",
    "\t\ts_new = True\n",
    "\treturn s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "toy=True\n",
    "sample = 0\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--job_array_task_id',\n",
    "                    help='default: ${SLURM_ARRAY_TASK_ID} or 1. When using job arrays, this will be set by the bash script by ${SLURM_ARRAY_TASK_ID} or set to 1, which will be substracted below by 1 for zero indexing')\n",
    "parser.add_argument('--toy', help='run quickly with less labels, parameters and splits')\n",
    "\n",
    "args = parser.parse_args()\n",
    "if args.job_array_task_id != None:\n",
    "\tsample = int(args.job_array_task_id) - 1\n",
    "    \n",
    "if args.toy!=None:\n",
    "\ttoy = str2boolean(args.toy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmO7Ma-l4zt2"
   },
   "outputs": [],
   "source": [
    "\n",
    "seed_value = None\n",
    "# silence NumbaPerformanceWarning\n",
    "import warnings\n",
    "import numba\n",
    "from numba.errors import NumbaPerformanceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=NumbaPerformanceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Mount GDrive and attach it to the colab for data I/O\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8026,
     "status": "ok",
     "timestamp": 1589991379197,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "6MA1IUdBLKcd",
    "outputId": "776d1fdf-5187-4985-bad2-6e9ec720d393"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, path ):\n",
    "    with open(path , 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6mNE2JeN6up"
   },
   "outputs": [],
   "source": [
    "# data_folder = '/content/drive/My Drive/ML4HC_Final_Project/data/input/'\n",
    "data_folder = './../../datum/reddit/input/'\n",
    "output_dir = './../../datum/reddit/output/umap/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits_2020 = ['COVID19_Support', 'personalfinance','relationships', 'addiction', 'EDAnonymous', 'adhd', 'autism', 'alcoholism', 'bipolarreddit', 'bpd','depression', 'anxiety',\n",
    "\t              'healthanxiety', 'lonely', 'schizophrenia', 'socialanxiety', 'suicidewatch']\n",
    "subreddits = ['addiction', 'EDAnonymous', 'adhd', 'autism', 'alcoholism', 'bipolarreddit', 'bpd','depression', 'anxiety',\n",
    "\t              'healthanxiety', 'lonely', 'schizophrenia', 'socialanxiety', 'suicidewatch']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: os.mkdir(output_dir)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3616,
     "status": "ok",
     "timestamp": 1589991418790,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "BTQH35SXHzWs",
    "outputId": "36a6b003-4438-4444-b131-3f3edbc71769"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x640 with 0 Axes>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x640 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn-bright')\n",
    "plt.figure(figsize=(10, 8), dpi= 80, facecolor='w', edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9P4PrbvNHzWw"
   },
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b, c):\n",
    "    assert len(a) == len(b) == len(c)\n",
    "    p = np.random.permutation(len(a))\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    return a[p], b[p], c[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5FqMlV8HzWz"
   },
   "outputs": [],
   "source": [
    "def scatter_plot(X_reduced, y, color_code, method, annotate = False, title = 'title', savefig = False,\n",
    "                 centers= None, dists=None):\n",
    "\n",
    "    plt.clf()\n",
    "    # Shuffle\n",
    "    X_reduced, y, color_code = unison_shuffled_copies(X_reduced, y, color_code)\n",
    "\n",
    "    # plot     \n",
    "#     c = LabelEncoder().fit_transform(color_code) #encode to integers\n",
    "#     c = [n*100 for n in c]\n",
    "    \n",
    "    data = pd.DataFrame(X_reduced, columns = ['x', 'y'])\n",
    "    data['label'] = y\n",
    "    color_order = np.unique(y)\n",
    "    print(color_order)\n",
    "#     color_order.sort()\n",
    "    facet = sns.lmplot(data=data, x='x', y='y', hue='label', hue_order=color_order,\n",
    "                   fit_reg=False, legend=True, legend_out=True, palette = 'colorblind', scatter_kws={\"s\": 1})\n",
    "#     scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1],c=c, alpha=0.6, s=1,cmap='Spectral')\n",
    "#     label_names = [str(n) for n in np.unique(y)] \n",
    "#     label_names.sort()\n",
    "#     plt.legend(handles=scatter.legend_elements()[0], labels=label_names)\n",
    "    if annotate:\n",
    "        for i, txt in enumerate(annotate_names):\n",
    "            plt.annotate(txt, (X_reduced[:, 0][i], X_reduced[:, 1][i]))\n",
    "\n",
    "    # Centroids\n",
    "    plt.scatter(\n",
    "        centers[:, 0], centers[:, 1],\n",
    "        s=10, marker='x',\n",
    "        c='magenta', edgecolor='black',\n",
    "        label='centroids')\n",
    "\n",
    "    # LINES\n",
    "    # missing one line     \n",
    "    # plt.plot(centers[:,0],centers[:,1], color = 'dodgerblue', linewidth=0.5) # this works    \n",
    "            \n",
    "        \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if savefig:\n",
    "        plt.savefig('./data/'+title.replace('/', '-')+'.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5lyIVyGIpGtM"
   },
   "outputs": [],
   "source": [
    "def run_umap(X=None, y=None, method = 'unsupervised', scaler=None, neighbor = 10, dist=0.1, metric='correlation', \n",
    "             color_code = None, annotate_names = None, annotate = False, test_set = True, title=None, \n",
    "             savefig = False, X_test=None, y_test=None, color_code_test = None, plot=True):\n",
    "    \n",
    "    reducer = umap.UMAP(n_components=dimension, n_neighbors = neighbor, min_dist=dist,metric=metric,random_state=seed_value) #, TSNE(n_components=k, random_state=seed_value), PCA(n_components=k, random_state=seed_value)]\n",
    "    reducer_name = 'umap' #, 'tsne', 'pca']\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('normalization', scaler),\n",
    "        ('reducer', reducer), ])\n",
    "\n",
    "    y_encoded = LabelEncoder().fit_transform(y)\n",
    "    if method == 'supervised':\n",
    "        X_reduced = pipeline.fit_transform(X, y_encoded)\n",
    "    elif method == 'metric_learning':\n",
    "        X_reduced = pipeline.fit_transform(X, y_encoded)\n",
    "        X_reduced_test = pipeline.transform(X_test)\n",
    "        \n",
    "    elif method == 'unsupervised':\n",
    "        X_reduced = pipeline.fit_transform(X)\n",
    "    \n",
    "    print('running kmeans...')\n",
    "    # Set k to amount of subreddits\n",
    "    k = len(np.unique(y))\n",
    "    # Fit kmeans\n",
    "    km = KMeans(n_clusters = k, random_state = seed_value).fit(X_reduced)\n",
    "    # Obtain labels for each data point\n",
    "    #     kmeans_labels = KMeans(n_clusters = k, random_state = seed_value).fit_predict(X_reduced)\n",
    "#     kmeans_labels = KMeans(n_clusters = k, random_state = seed_value).fit_predict(X_reduced)\n",
    "    # Obtain euclidean distance between centroids\n",
    "    centers = km.cluster_centers_\n",
    "    # find centroid labels      \n",
    "    closest, _ = pairwise_distances_argmin_min(centers, X_reduced)\n",
    "    data = pd.DataFrame(X_reduced, columns = ['x1', 'x2'])\n",
    "    data['label'] = y\n",
    "    centers_labels = list(data.loc[closest].label)\n",
    "    \n",
    "    # Plot in 2D\n",
    "    if plot:\n",
    "        assert dimension == 2 \n",
    "        if method == 'metric_learning':\n",
    "            # train: first time point\n",
    "            scatter_plot(X_reduced, y, color_code, method, annotate = annotate, title = 'First time step (train set)', savefig = savefig )\n",
    "            # test: next time points            \n",
    "            scatter_plot(X_reduced_test, y_test, color_code_test, method, annotate = annotate, title = title, savefig = savefig )\n",
    "            \n",
    "        else:\n",
    "            scatter_plot(X_reduced, y, color_code, method, annotate = annotate, title = title, savefig = savefig, centers=centers)\n",
    "    if method == 'metric_learning':\n",
    "        return X_reduced, X_reduced_test\n",
    "    else:\n",
    "        return X_reduced, centers, centers_labels\n",
    "\n",
    "def scatter_X_reduced(X_reduced, color_code, annotate_names):\n",
    "    plt.clf()\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=color_code, alpha=0.8, s=6)\n",
    "    if annotate:\n",
    "        for i, txt in enumerate(annotate_names):\n",
    "            plt.annotate(txt, (X_reduced[:, 0][i], X_reduced[:, 1][i]))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def gridsearch_sets(metrics, n_neighbors, min_dist, n_dimensions, scalers, repeat):\n",
    "    gridsearch  = []\n",
    "    for metric in metrics:\n",
    "        for neighbor in n_neighbors:\n",
    "            for dist in min_dist:\n",
    "                for dimension in n_dimensions:\n",
    "                    for scaler in scalers:\n",
    "                        for r in repeat:\n",
    "                            gridsearch.append([metric,neighbor,dist,dimension,scaler, r])\n",
    "    return gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IoS2Yk-_eRWm"
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.spatial import ConvexHull\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwUC6Lh5S0ms"
   },
   "source": [
    "## Load data with our without subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eu0_cxdW0zVW"
   },
   "outputs": [],
   "source": [
    "# subreddits = ['EDAnonymous', 'addiction', 'adhd', 'alcoholism', 'anxiety',\n",
    "#  'bipolarreddit', 'bpd',  'depression',  'healthanxiety',\n",
    "#        'jokes', 'legaladvice', 'meditation', 'mentalhealth',\n",
    "#        'mentalillness', 'mindfulness', 'paranoia', \n",
    "#        'personalfinance','ptsd', 'schizophrenia', 'socialanxiety', \n",
    "#        'suicidewatch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4zNHZs-ot4X"
   },
   "outputs": [],
   "source": [
    "\n",
    "def subsample_df(df, subsample):\n",
    "\tif type(subsample) == float:\n",
    "\t\tsubsample = int(df.shape[0]*subsample)\n",
    "\tdf = df.reset_index(drop=True)\n",
    "\tdf2 = df.loc[np.random.choice(df.index,subsample, replace=False)]\n",
    "\treturn df2\n",
    "\n",
    "\n",
    "\n",
    "# def subsample_df(df, subsample, overN):\n",
    "#   if df.shape[0] > subsample_subreddits_overN:\n",
    "#     subsample_int = int(df.shape[0]*subsample)\n",
    "#     df = df.loc[np.random.choice(df.index,subsample_int, replace=False)]\n",
    "#   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isJNT9UwpGs6"
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def clean(df):\n",
    "    # remove author duplicates and shuffle so we dont keep only first posts in time\n",
    "    reddit_data = df.sample(frac=1) #shuffle\n",
    "    reddit_data = reddit_data.drop_duplicates(subset='author', keep='first')\n",
    "    reddit_data  = reddit_data [~reddit_data.author.str.contains('|'.join(['bot', 'BOT', 'Bot']))] # There is at least one bot per subreddit\n",
    "    reddit_data = reddit_data[~reddit_data.post.str.contains('|'.join(['quote', 'QUOTE', 'Quote']))] # Remove posts in case quotes are long\n",
    "    reddit_data = reddit_data.reset_index(drop=True)\n",
    "    return reddit_data\n",
    "\n",
    "\n",
    "def load_reddit(subreddits, data_folder='./', subsample = 5600,pre_or_post = 'pre'):\n",
    "    # subsample = 5600 #False, int for balanced, or 0.1 for unbalanced proportion, 5600\n",
    "    # Careful: if you add COVID19_support and it does not exist in the first time step, then this will confuse metric learning\n",
    "    subreddits.sort()\n",
    "    # Main features\n",
    "    # Load first subreddit to build DF\n",
    "    reddit_data = pd.read_csv(data_folder + 'feature_extraction/'+subreddits[0]+'_{}_features.csv'.format(pre_or_post), index_col=False)        \n",
    "    #     Clean\n",
    "    reddit_data = clean(reddit_data)\n",
    "    \n",
    "    # Concat tfidf features\n",
    "    reddit_data_tfidf = pd.read_csv(data_folder + 'tfidf_vector/'+subreddits[0]+'_{}_tfidf256.csv'.format(pre_or_post), index_col=False)\n",
    "    reddit_data = reddit_data.merge(reddit_data_tfidf) ##inner is default, will elimante rows not shared on shared cols (post, author, etc, date)\\n\",\n",
    "\n",
    "    # remove jan and feb data from covid19_support because there's not enough data and if not kmeans will assign two centroids to another larger subreddit\n",
    "    days = np.unique(reddit_data.date)\n",
    "    days_jan_feb = [n for n in days if '2020/01' in n or '2020/02' in n]\n",
    "    days_jan_feb\n",
    "    if subreddits[0]=='COVID19_support' and pre_or_post == 'post':\n",
    "      reddit_data = reddit_data[~reddit_data.date.isin(days_jan_feb)]\n",
    "\n",
    "    # Subsample to int or subsample float\n",
    "    print(reddit_data.shape)\n",
    "    if subsample and subreddits[0] !='COVID19_support':\n",
    "        reddit_data = subsample_df(reddit_data, subsample)\n",
    "        print(reddit_data.shape)\n",
    "\n",
    "    # Add next subreddits\n",
    "    for i in np.arange(1, len(subreddits)):\n",
    "        print('===')\n",
    "        print(subreddits[i])\n",
    "        new_data = pd.read_csv(data_folder + 'feature_extraction/'+subreddits[i]+'_{}_features.csv'.format(pre_or_post), index_col=False)\n",
    "        # Clean\n",
    "        new_data  = clean(new_data )\n",
    "\n",
    "        new_data_tfidf = pd.read_csv(data_folder + 'tfidf_vector/'+subreddits[i]+'_{}_tfidf256.csv'.format(pre_or_post), index_col=False)\n",
    "        new_data = new_data.merge(new_data_tfidf) ##inner is default, will elimante rows not shared on shared cols (post, author, etc, date)\\n\",\n",
    "        if subreddits[i]=='COVID19_support' and pre_or_post == 'post':\n",
    "              reddit_data = reddit_data[~reddit_data.date.isin(days_jan_feb)]\n",
    "        print(new_data.shape)\n",
    "        if subsample and subreddits[i] !='COVID19_support':\n",
    "            new_data = subsample_df(new_data, subsample)\n",
    "            print(new_data.shape)\n",
    "        reddit_data = pd.concat([reddit_data, new_data], axis=0)\n",
    "\n",
    "    return reddit_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "em1pocGXxzwH"
   },
   "outputs": [],
   "source": [
    "# reddit_data = load_reddit(subreddits, data_folder=data_folder, subsample = 5600,pre_or_post = 'pre')\n",
    "\n",
    "# counts = reddit_data.groupby([\"subreddit\", \"date\"]).size().reset_index(name='count')\n",
    "# print('counts per day:')\n",
    "# for sr in subreddits:\n",
    "#   counts_d = counts[counts.subreddit == sr].mean()\n",
    "#   print(sr, ': ', np.round(float(counts_d),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5RX5VmI_9jh"
   },
   "source": [
    "## Find optimal params (optimize on silhouette score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4647330,
     "status": "ok",
     "timestamp": 1589996667737,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "nf1tcL9F_7kt",
    "outputId": "01d0ff4b-708b-4124-d114-576c0f5463ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100 ==========================================\n",
      "(7401, 351)\n",
      "(100, 351)\n",
      "===\n",
      "addiction\n",
      "(4778, 351)\n",
      "(100, 351)\n",
      "===\n",
      "adhd\n",
      "(18467, 351)\n",
      "(100, 351)\n",
      "===\n",
      "alcoholism\n",
      "(4110, 351)\n",
      "(100, 351)\n",
      "===\n",
      "anxiety\n",
      "(21931, 351)\n",
      "(100, 351)\n",
      "===\n",
      "autism\n",
      "(5137, 351)\n",
      "(100, 351)\n",
      "===\n",
      "bipolarreddit\n",
      "(3051, 351)\n",
      "(100, 351)\n",
      "===\n",
      "bpd\n",
      "(12869, 351)\n",
      "(100, 351)\n",
      "===\n",
      "depression\n",
      "(23804, 351)\n",
      "(100, 351)\n",
      "===\n",
      "healthanxiety\n",
      "(4767, 351)\n",
      "(100, 351)\n",
      "===\n",
      "lonely\n",
      "(12741, 351)\n",
      "(100, 351)\n",
      "===\n",
      "schizophrenia\n",
      "(4710, 351)\n",
      "(100, 351)\n",
      "===\n",
      "socialanxiety\n",
      "(13503, 351)\n",
      "(100, 351)\n",
      "===\n",
      "suicidewatch\n",
      "(22288, 351)\n",
      "(100, 351)\n",
      "\n",
      "0 out of 8\n",
      "====metric: euclidean,  100 neighbor (low=maintain local structure),  0 dist (low=tightly packed),  2D======\n",
      "running umap...\n",
      "running kmeans...\n",
      "runnning metrics\n",
      "\n",
      "1 out of 8\n",
      "====metric: euclidean,  100 neighbor (low=maintain local structure),  0.1 dist (low=tightly packed),  2D======\n",
      "running umap...\n",
      "running kmeans...\n",
      "runnning metrics\n",
      "\n",
      "2 out of 8\n",
      "====metric: euclidean,  200 neighbor (low=maintain local structure),  0 dist (low=tightly packed),  2D======\n",
      "running umap...\n",
      "running kmeans...\n",
      "runnning metrics\n",
      "\n",
      "3 out of 8\n",
      "====metric: euclidean,  200 neighbor (low=maintain local structure),  0.1 dist (low=tightly packed),  2D======\n",
      "running umap...\n",
      "running kmeans...\n",
      "runnning metrics\n",
      "\n",
      "4 out of 8\n",
      "====metric: cosine,  100 neighbor (low=maintain local structure),  0 dist (low=tightly packed),  2D======\n",
      "running umap...\n",
      "running kmeans...\n",
      "runnning metrics\n",
      "\n",
      "5 out of 8\n",
      "====metric: cosine,  100 neighbor (low=maintain local structure),  0.1 dist (low=tightly packed),  2D======\n",
      "running umap...\n",
      "running kmeans...\n",
      "runnning metrics\n",
      "\n",
      "6 out of 8\n",
      "====metric: cosine,  200 neighbor (low=maintain local structure),  0 dist (low=tightly packed),  2D======\n",
      "running umap...\n",
      "running kmeans...\n",
      "runnning metrics\n",
      "\n",
      "7 out of 8\n",
      "====metric: cosine,  200 neighbor (low=maintain local structure),  0.1 dist (low=tightly packed),  2D======\n",
      "running umap...\n",
      "running kmeans...\n",
      "runnning metrics\n"
     ]
    }
   ],
   "source": [
    "# Plot 2D for each timestep\n",
    "\n",
    "results = {}\n",
    "sample_sizes = [3000,3000,3000,0.1,0.5, False]\n",
    "sample_names = ['3000_0','3000_1','3000_2','1','5', 'all']\n",
    "if toy:\n",
    "    sample_sizes = [100,0.01]\n",
    "    sample_names = ['100','01']\n",
    "\n",
    "#     defined through argparse\n",
    "i = sample_sizes[sample]\n",
    "name = sample_names[sample]\n",
    "    \n",
    "# for i, name in zip(sample_sizes, sample_names):\n",
    "print('\\n\\n{} =========================================='.format(i))\n",
    "results_i = []\n",
    "reddit_data = load_reddit(subreddits, data_folder=data_folder, subsample = i,pre_or_post = 'pre')\n",
    "features = list(reddit_data.columns)\n",
    "features = [n for n in features if n not in ['subreddit','author','date','post']]\n",
    "#   print('double check features: ',features)\n",
    "X = reddit_data[features].values\n",
    "y = reddit_data.subreddit.values\n",
    "\n",
    "method = 'supervised'# 'metric_learning', 'supervised', 'unsupervised'\n",
    "savefig = False\n",
    "plot=False\n",
    "annotate = False\n",
    "annotate_names = False\n",
    "run_Ntimesteps = 1# len(X)#len(X) #1,2 ... len(X) \n",
    "color_code = y.copy()\n",
    "\n",
    "\n",
    "# Set up gridsearch\n",
    "n_dimensions =  [2,] #4,8,16,32,64,128] # https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "n_neighbors = [100,200] #[50,100,200] # CANNOT USE MORE THAN N participants, default=10 Lower values seem to work better in 2D. This means that low values of n_neighbors will force UMAP to concentrate on very local structure\n",
    "min_dist = [0, 0.1]  # default=0.1. Lower values seem to work better in 2D. controls how tightly UMAP is allowed to pack points together. Larger values of min_dist will prevent UMAP from packing point together and will focus instead on the preservation of the broad topological structure instead.\n",
    "metrics = ['euclidean', 'cosine'] #,'cosine'] # cosine adds points?\n",
    "repeat_n = 1\n",
    "repeat = [1]*repeat_n # to check how much randomness is affecting results, one can repeat\n",
    "scalers = [StandardScaler()]\n",
    "\n",
    "gridsearch= gridsearch_sets(metrics, n_neighbors, min_dist, n_dimensions, scalers, repeat)\n",
    "for j, (metric,neighbor,dist,dimension,scaler, r) in enumerate(gridsearch):\n",
    "    print('\\n{} out of {}'.format(j, len(gridsearch)))\n",
    "    print('====metric: {},  {} neighbor (low=maintain local structure),  {} dist (low=tightly packed),  {}D======'.format(metric,neighbor,dist,dimension))\n",
    "    title = '{} metric, neighbor {}, {} dist'.format(metric,neighbor,dist)\n",
    "    print('running umap...')         \n",
    "    X_reduced, centers, centers_labels  = run_umap(X = X, y = y, method = method, scaler=  scaler, neighbor = neighbor , dist=dist, metric=metric, \n",
    "        color_code = color_code, annotate_names = annotate_names, annotate = annotate, title=title,\n",
    "        savefig = savefig, plot = plot)\n",
    "\n",
    "    print('runnning metrics')\n",
    "    # compute distances\n",
    "    dists = euclidean_distances(centers)\n",
    "    dists_df = pd.DataFrame(dists, columns = centers_labels, index=centers_labels)\n",
    "    # Compute silhouette score\n",
    "    sil_score = silhouette_score(X_reduced, y)\n",
    "\n",
    "    # Compute convex hull\n",
    "    hull = ConvexHull(centers)\n",
    "    hull_area = hull.volume #volume is area in 2D\n",
    "    results_i.append([metric,neighbor,dist,sil_score, dists_df.mean().mean(),dists_df.mean().T['socialanxiety'],dists_df.mean().T['suicidewatch'], hull_area])\n",
    "\n",
    "results_gs = pd.DataFrame(results_i)\n",
    "results_gs.columns = ['metric', 'neighbor','dist', 'sil_score', 'mean_dist', 'socialanxiety_dist', 'suicide_dist', 'convexhull']\n",
    "results_gs = results_gs.sort_values('sil_score')\n",
    "results[name] = results_gs\n",
    "timestamp = datetime.datetime.now().isoformat()\n",
    "save_obj(results,output_dir+f'umap_gs_results_sample_{name}_{timestamp}.pkl')\n",
    "\n",
    "\n",
    "        \n",
    "#         # Upper triangle\n",
    "#         tri_dists = dists[np.triu_indices(len(centers_labels), 1)]\n",
    "#         # Stats of upper triangle to measure overall convergence divergence\n",
    "#         max_dist, avg_dist, min_dist = tri_dists.max(), tri_dists.mean(), tri_dists.min()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmLC1EKf0ViI"
   },
   "source": [
    "\n",
    "## Build X, y by grouping data by timestep (e.g., per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1945,
     "status": "ok",
     "timestamp": 1589925866853,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "ySVm5rkkHzXN",
    "outputId": "5791e1ae-8522-400a-c11c-a27e07f29f93"
   },
   "source": [
    "len(days)/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSPzD-tzNCnO"
   },
   "source": [
    "output_dir = '/content/drive/My Drive/ML4HC_Final_Project/data/output/supervised_clustering/'\n",
    "timestep = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CAVxALWmSye5"
   },
   "source": [
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in range(0,len(days),timestep)[:-1]:\n",
    "  days_week = days[i:i+timestep]\n",
    "  df_week = reddit_data[reddit_data.date.isin(days_week)]\n",
    "  df_week_feature_cols = df_week[features].values\n",
    "  df_week_y = list(df_week.subreddit)\n",
    "  X.append(df_week_feature_cols)\n",
    "  y.append(df_week_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NT7TwYLrCjOr"
   },
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13361,
     "status": "ok",
     "timestamp": 1589929606148,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "-y6xdHC_HzXW",
    "outputId": "7ba2356c-83ab-4fce-ddd7-b94b1b35db1f"
   },
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12915,
     "status": "ok",
     "timestamp": 1589929606149,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "DRgGrf5mHzXY",
    "outputId": "b32102d1-631f-4a89-eba7-26d504bcc2ec"
   },
   "source": [
    "days_week = days[::timestep]\n",
    "len(days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rs6uIA_1agoh"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0KaiFXP-681"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3352610,
     "status": "ok",
     "timestamp": 1589932947013,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "Q1F7gyENpGxB",
    "outputId": "a0edc858-8744-40a6-a382-8ee1fdeba1c6"
   },
   "source": [
    "# Plot 2D for each timestep\n",
    "\n",
    "\n",
    "method = 'supervised'# 'metric_learning', 'supervised', 'unsupervised'\n",
    "savefig = False\n",
    "plot=True\n",
    "annotate = False\n",
    "annotate_names = False\n",
    "run_Ntimesteps = len(X)# len(X)#len(X) #1,2 ... len(X) \n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, (X_i, y_i) in enumerate(zip(X[:run_Ntimesteps],y[:run_Ntimesteps])):\n",
    "    if method  == 'metric_learning' and i==0:\n",
    "        print('doing metric learning, train on first sample and test on rest')\n",
    "        color_code = y[0].copy()\n",
    "        continue\n",
    "\n",
    "    print(days_week[i])\n",
    "    if method == 'metric_learning':\n",
    "        color_code_test = y_i.copy()\n",
    "    else:\n",
    "        color_code = y_i.copy()\n",
    "    # groups = LabelEncoder().fit_transform(groups)\n",
    "    n_dimensions =  [2,]#4,8,16,32,64,128] # https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "    n_neighbors = [200,100]#[2,10,20,50,100,200]#   #[8,16,24,32,40,48,56, 64]#15,20] # CANNOT USE MORE THAN N participants, default=10 Lower values seem to work better in 2D. This means that low values of n_neighbors will force UMAP to concentrate on very local structure\n",
    "    min_dist = [0]#[0,0.1,0.25,0.5,0.8, 0.99]#[0.01, 0.1, 0.5, 1]#[0, 0.001,0.01, 0.1, 0.5]#0.25, 0.5, 0.75]  # default=0.1. Lower values seem to work better in 2D. controls how tightly UMAP is allowed to pack points together. Larger values of min_dist will prevent UMAP from packing point together and will focus instead on the preservation of the broad topological structure instead.\n",
    "    metrics =['euclidean', 'correlation', 'cosine']# ['correlation', 'euclidean'] # cosine adds points?\n",
    "    repeat_n = 1\n",
    "    repeat = [1]*repeat_n # to check how much randomness is affecting results, one can repeat\n",
    "    scalers = [StandardScaler()]\n",
    "\n",
    "    gridsearch= gridsearch_sets(metrics, n_neighbors, min_dist, n_dimensions, scalers, repeat)\n",
    "    for j, (metric,neighbor,dist,dimension,scaler, r) in enumerate(gridsearch):\n",
    "        print('====metric: {},  {} neighbor (low=maintain local structure),  {} dist (low=tightly packed),  {}D======'.format(metric,neighbor,dist,dimension))\n",
    "        if method == 'metric_learning':\n",
    "            # need to add train and test (the last arguments)             \n",
    "            X_reduced, X_reduced_test = run_umap(X = X[0], y = y[0], method=method,  scaler = scaler, neighbor = neighbor , dist=dist, metric='correlation', \n",
    "                  color_code = color_code, annotate_names = annotate_names, annotate = annotate, title=days_week[i],\n",
    "                 savefig = savefig, X_test = X_i, y_test=y_i, color_code_test = color_code_test, plot = plot)\n",
    "        else:    \n",
    "            X_reduced, centers, centers_labels  = run_umap(X = X_i, y = y_i, method = method, scaler=  scaler, neighbor = neighbor , dist=dist, metric='correlation', \n",
    "                  color_code = color_code, annotate_names = annotate_names, annotate = annotate, title=days_week[i],\n",
    "                 savefig = savefig, plot = plot)\n",
    "        # Measure distances             \n",
    "        # TODO: add Hausdorff distance\n",
    "        # TODO: add Hausdorff distance\n",
    "        dists = euclidean_distances(centers)\n",
    "        dists_df = pd.DataFrame(dists, columns = centers_labels, index=centers_labels)\n",
    "        print(dists_df)\n",
    "        # Compute silhouette score\n",
    "        sil_score = silhouette_score(X_reduced, y_i)\n",
    "        \n",
    "        results[(days_week[i],metric,neighbor,dist)] = dists_df\n",
    "\n",
    "\n",
    "        \n",
    "#         # Upper triangle\n",
    "#         tri_dists = dists[np.triu_indices(len(centers_labels), 1)]\n",
    "#         # Stats of upper triangle to measure overall convergence divergence\n",
    "#         max_dist, avg_dist, min_dist = tri_dists.max(), tri_dists.mean(), tri_dists.min()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErQM27j7aipp"
   },
   "source": [
    "\n",
    "# save_obj(results,output_dir+'supervised_clustering_{}timestep_v3'.format(timestep) )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17813,
     "status": "ok",
     "timestamp": 1589894577139,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "I0rp5pCGLt7V",
    "outputId": "8c1ab8b9-1af7-4d8a-fb00-e10be79b43dc"
   },
   "source": [
    "results = load_obj(output_dir+'supervised_clustering_{}timestep_v3'.format(timestep))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1642,
     "status": "ok",
     "timestamp": 1589894650572,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "e3T1Bpo6NJNf",
    "outputId": "fd198cf1-9d86-403a-aa18-74632ee36f5a"
   },
   "source": [
    "# plot distance between all subreddits and anxiety for \n",
    "\n",
    "days = np.unique([key[0] for key in results.keys()])\n",
    "\n",
    "results_anxiety = []\n",
    "\n",
    "\n",
    "\n",
    "print(len(items_day))  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LGWHUmV6PwLi"
   },
   "source": [
    "# Make sure there is the right amount. For instance, COVID19 appears in March so if there are 9 subreddits, then there should be 9 until March when there should be 10. \n",
    "unique_index =[]\n",
    "day_sr = {}\n",
    "for day in days:\n",
    "    items_day = [item for item in results.items() if day in item[0]]\n",
    "    sr = []\n",
    "    for params in items_day:\n",
    "      len_index = len(params[1].index)\n",
    "      len_unique_index = len(np.unique(params[1].index))\n",
    "      unique_index.append(len_unique_index)\n",
    "      if len_unique_index<len_index: #change depending on subreddits - COVID19 which only appears later\n",
    "        continue\n",
    "      sr.append(params[1]['healthanxiety'])\n",
    "    day_sr[day] = sr\n",
    "    \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A16x3nJOMDNY"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbyUcojkJLGb"
   },
   "source": [
    "\n",
    "sr_mean = []\n",
    "sr_std = []\n",
    "\n",
    "for day in days: \n",
    "  sr = day_sr.get(day)\n",
    "  sr_df = pd.concat(sr, axis=1)\n",
    "  sr_mean.append(sr_df.T.mean())\n",
    "  sr_std.append(sr_df.T.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1960,
     "status": "ok",
     "timestamp": 1589897978603,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "MsB9u-ScMOP9",
    "outputId": "7e0664f1-6fc1-4c91-bcb9-a8fbaf5dde05"
   },
   "source": [
    "days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1887,
     "status": "ok",
     "timestamp": 1589897979367,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "yYl3firDMD8e",
    "outputId": "0f4d073f-5013-49e5-8772-5bcc1b9c352a"
   },
   "source": [
    "sr_mean_df = pd.concat(sr_mean, axis=1)\n",
    "sr_mean_df.columns=days\n",
    "sr_mean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4066,
     "status": "ok",
     "timestamp": 1589897982480,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "uqZNEqcXJ5ND",
    "outputId": "1414d6cd-f4d3-45ca-fbed-181c6aa16710"
   },
   "source": [
    "sr_mean_df.T.plot.line(subplots=True, figsize = (14,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11077,
     "status": "ok",
     "timestamp": 1589898411448,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "CE9jA5pQKC9s",
    "outputId": "2e365d2c-d1f5-4999-8d96-f0c59e1b25d6"
   },
   "source": [
    "plt.style.use('seaborn-bright')\n",
    "plt.figure(figsize=(14,20), dpi= 80, facecolor='w', edgecolor='k')\n",
    "for sr2 in subreddits[1:]:\n",
    "  sr1_sr2_df = pd.DataFrame(sr_mean_df.T[sr2].T)\n",
    "  sr1_sr2_df['days'] = sr1_sr2_df.index\n",
    "  sr1_sr2_df['range'] = range(len(sr1_sr2_df['days'].values))\n",
    "  sns.lmplot(x='range',y=sr2,data=sr1_sr2_df,fit_reg=True, order=3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1589898221119,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "MnGZ4TH1UN89",
    "outputId": "11c04a44-da8b-4994-e104-172eda462d34"
   },
   "source": [
    "sr1_sr2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 787,
     "status": "ok",
     "timestamp": 1589898150091,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "sh0nFeX2UG84",
    "outputId": "6102ad41-9fd9-434b-ee3d-3cce328d2a65"
   },
   "source": [
    "sr1_sr2_df = pd.DataFrame(sr_mean_df.T[sr2].T)\n",
    "sr1_sr2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FsVf2hemS9KH"
   },
   "source": [
    "sr1_sr2_df['days'] = sr1_sr2_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4588,
     "status": "ok",
     "timestamp": 1589897214967,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "1NkTA24cQgQi",
    "outputId": "bf4473f1-0636-457c-f7cc-e0b11e5ccc90"
   },
   "source": [
    "sr1_sr2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 847,
     "status": "error",
     "timestamp": 1589897107385,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "WuDQOuvEPXsU",
    "outputId": "8e989f59-4d5b-4542-ef28-4076d8bb7b9c"
   },
   "source": [
    "sr1_sr2_df\n",
    "sr1_sr2_df = sr1_sr2_df.reset_index()\n",
    "sr1_sr2_df.columns = ['days', 'dist_to_sr1']\n",
    "sr1_sr2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 850,
     "status": "ok",
     "timestamp": 1589894767175,
     "user": {
      "displayName": "Daniel Low",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1lC4GGPhwI0Hi1dt3CjW-aYrHlTFD3EewcOttPw=s64",
      "userId": "13357881499464447434"
     },
     "user_tz": 180
    },
    "id": "fYrPzprBG7GA",
    "outputId": "725d0a90-0a11-4736-8784-8a2d8d84ba4b"
   },
   "source": [
    "items_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_zVKOFwPJba"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vp036sCq2Zxi"
   },
   "source": [
    "# SKIP DFS WHERE THERE IS A REPEATED COLUMN/INDEX. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U125CCq6HzXd"
   },
   "source": [
    "# subreddits = ['mindfulness', 'healthanxiety', 'anxiety']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mypJXzD_HzXf"
   },
   "source": [
    "# Measure distances             \n",
    "dists = euclidean_distances(centers)\n",
    "dists_df = pd.DataFrame(dists, columns = centers_labels, index=centers_labels)\n",
    "# Upper triangle\n",
    "tri_dists = dists[np.triu_indices(len(centers_labels), 1)]\n",
    "# Stats of upper triangle to measure overall convergence divergence\n",
    "max_dist, avg_dist, min_dist = tri_dists.max(), tri_dists.mean(), tri_dists.min()\n",
    "dists_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bari5mqCHzXi"
   },
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3dTZsMcHzXk"
   },
   "source": [
    "## How much do they agree?\n",
    " evaluating the adjusted Rand score and adjusted mutual information for this clustering as compared with the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCUppbtbHzXk"
   },
   "source": [
    "\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "(\n",
    "    adjusted_rand_score(y_i, kmeans_labels),\n",
    "    adjusted_mutual_info_score(y_i, kmeans_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKnxdl1rHzXn"
   },
   "source": [
    "### ToDo: Can I plot distance on line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktFRk-WLHzXo"
   },
   "source": [
    "# !pip install matplotlib-label-lines\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import loglaplace,chi2\n",
    "\n",
    "from labellines import labelLine, labelLines\n",
    "\n",
    "X = np.linspace(0,1,500)\n",
    "A = [1,2,5,10,20]\n",
    "funcs = [np.arctan,np.sin,loglaplace(4).pdf,chi2(5).pdf]\n",
    "\n",
    "# plt.subplot(321)\n",
    "for a in A:\n",
    "    plt.plot(X,np.arctan(a*X),label=str(a))\n",
    "\n",
    "labelLines(plt.gca().get_lines(),zorder=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87rP1jQ7HzXq"
   },
   "source": [
    "# Plot 2D for each timestep\n",
    "# https://stackoverflow.com/questions/52911890/labeling-distance-between-points-on-python-plot\n",
    "\n",
    "# method = 'supervised'# 'metric_learning', 'supervised', 'unsupervised'\n",
    "# savefig = False\n",
    "# plot=True\n",
    "# annotate = False\n",
    "# annotate_names = False\n",
    "# run_Ntimesteps = 1 #len(X) \n",
    "\n",
    "# # for each X_i, data in each timestep:\n",
    "# for i, (X_i, y_i) in enumerate(zip(X[:run_Ntimesteps],y[:run_Ntimesteps])):\n",
    "#     if method  == 'metric_learning' and i==0:\n",
    "#         print('doing metric learning, train on first sample and test on rest')\n",
    "#         color_code = y[0].copy()\n",
    "#         continue\n",
    "\n",
    "#     print(days_week[i])\n",
    "#     if method == 'metric_learning':\n",
    "#         color_code_test = y_i.copy()\n",
    "#     else:\n",
    "#         color_code = y_i.copy()\n",
    "#     # groups = LabelEncoder().fit_transform(groups)\n",
    "#     n_dimensions =  [2,]#4,8,16,32,64,128] # https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "#     n_neighbors = [48]#[8,16,24,32,40,48,56, 64]#15,20] # CANNOT USE MORE THAN N participants, default=10 Lower values seem to work better in 2D. This means that low values of n_neighbors will force UMAP to concentrate on very local structure\n",
    "#     min_dist = [0.1]#[0.01, 0.1, 0.5, 1]#[0, 0.001,0.01, 0.1, 0.5]#0.25, 0.5, 0.75]  # default=0.1. Lower values seem to work better in 2D. controls how tightly UMAP is allowed to pack points together. Larger values of min_dist will prevent UMAP from packing point together and will focus instead on the preservation of the broad topological structure instead.\n",
    "#     metrics = ['correlation'] # cosine adds points?\n",
    "#     repeat_n = 1\n",
    "#     repeat = [1]*repeat_n # to check how much randomness is affecting results, one can repeat\n",
    "#     scalers = [StandardScaler()]\n",
    "\n",
    "#     gridsearch= gridsearch_sets(metrics, n_neighbors, min_dist, n_dimensions, scalers, repeat)\n",
    "#     for metric,neighbor,dist,dimension,scaler, r in gridsearch:\n",
    "#         print('====metric: {},  {} neighbor (low=maintain local structure),  {} dist (low=tightly packed),  {}D======'.format(metric,neighbor,dist,dimension))\n",
    "#         if method == 'metric_learning':\n",
    "#             # need to add train and test (the last arguments)             \n",
    "#             plot_lines(X)\n",
    "#         else:    \n",
    "#             X_reduced = plot_lines(X)\n",
    "\n",
    "\n",
    "\n",
    "# distances = []\n",
    "# x_0, y_0 = a[0], b[0]\n",
    "# for i in range(len(a))[:-1]:\n",
    "#     if i == len(a):\n",
    "#             two_points = np.array([[a[i],b[i]],\n",
    "#                   [a[0],b[0]],\n",
    "#                  ])\n",
    "#     else:\n",
    "#         two_points = np.array([[a[i],b[i]],\n",
    "#                       [a[i+1],b[i+1]],\n",
    "#                      ])\n",
    "#     print(two_points)\n",
    "#     d = euclidean_distances(two_points)\n",
    "#     print(d[0][1])\n",
    "#     distances.append(d[0][1])\n",
    "    \n",
    "\n",
    "    \n",
    "    # Plot distance on line, couldn't solve:     \n",
    "\n",
    "    #     import itertools\n",
    "    #     combs = list(itertools.combinations(centers,4))\n",
    "    #     combs = [n for i in combs for n in i]\n",
    "    #     combs = np.array(combs)\n",
    "    #     combs\n",
    "    #     dists = euclidean_distances(combs)\n",
    "    \n",
    "    #     for i in range(len(a)):\n",
    "    #         if i == len(a):\n",
    "    #             break\n",
    "    #         plt.plot(a[i:i+1], b[i:i+1])\n",
    "    #     plt.text(0.9, 0.2, 'cos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pf4WhafPHzXt"
   },
   "source": [
    "## Output gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMcAiJtjpGxT"
   },
   "source": [
    "input_dir = '/Users/danielmlow/Dropbox (MIT)/libs/reddit/data/timestep_{}/'.format(timestep)\n",
    "filenames = os.listdir(input_dir)\n",
    "# images = []\n",
    "# for filename in filenames:\n",
    "#     images.append(imageio.imread(input_dir+filename))\n",
    "# imageio.mimsave(input_dir+'supervised.gif', images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pz7CxEy5pGxg"
   },
   "source": [
    "import imageio\n",
    "with imageio.get_writer(input_dir+'supervised.gif', mode='I') as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(input_dir+filename)\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnzM_IBBpGx4"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "reddit_cluster.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
