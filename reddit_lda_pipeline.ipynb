{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9hk7oG4pe6_"
   },
   "source": [
    "## Steps to mount drive to access folders and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "BhBMiZhBeACq",
    "outputId": "f8f4edff-a488-42aa-e5a6-89c11dd137d7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from gensim.models import Phrases\n",
    "from gensim.test.utils import datapath\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "huNrVL1lpyDG"
   },
   "source": [
    "## Extract out documents from Pre-Pandemic Data and Pre-Process\n",
    "\n",
    "Procedure adapted from: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bw1tYoCrd6Is"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We extract out a sampling of posts from pre- and mid-pandemic mental health subreddits.\n",
    "These are stemmed and lemmatized. They are then made into a dictionary and are used\n",
    "to create a bag of words corpus. This corpus and dictionary are used to form the LDA model.\n",
    "\n",
    "'''\n",
    "\n",
    "data_folder = './data/input/reddit_mental_health_dataset'\n",
    "output_path = \"./data/output/\"\n",
    "\n",
    "pre_pandemic = True\n",
    "\n",
    "pre_files = ['suicidewatch_pre_features_tfidf_256.csv',\n",
    "        'depression_pre_features_tfidf_256.csv',\n",
    "        'ptsd_pre_features_tfidf_256.csv',\n",
    "        'anxiety_pre_features_tfidf_256.csv',\n",
    "        'socialanxiety_pre_features_tfidf_256.csv',\n",
    "        'bipolarreddit_pre_features_tfidf_256.csv',\n",
    "        'bpd_pre_features_tfidf_256.csv',\n",
    "        'schizophrenia_pre_features_tfidf_256.csv',\n",
    "        'EDAnonymous_pre_features_tfidf_256.csv',\n",
    "        'alcoholism_pre_features_tfidf_256.csv',\n",
    "        'addiction_pre_features_tfidf_256.csv',\n",
    "        'adhd_pre_features_tfidf_256.csv',\n",
    "        'autism_pre_features_tfidf_256.csv',\n",
    "        'lonely_pre_features_tfidf_256.csv']\n",
    "\n",
    "mid_files = ['suicidewatch_post_features_tfidf_256.csv',\n",
    "        'depression_post_features_tfidf_256.csv',\n",
    "        'ptsd_post_features_tfidf_256.csv',\n",
    "        'anxiety_post_features_tfidf_256.csv',\n",
    "        'socialanxiety_post_features_tfidf_256.csv',\n",
    "        'bipolarreddit_post_features_tfidf_256.csv',\n",
    "        'bpd_post_features_tfidf_256.csv',\n",
    "        'schizophrenia_post_features_tfidf_256.csv',\n",
    "        'EDAnonymous_post_features_tfidf_256.csv',\n",
    "        'alcoholism_post_features_tfidf_256.csv',\n",
    "        'addiction_post_features_tfidf_256.csv',\n",
    "        'adhd_post_features_tfidf_256.csv',\n",
    "        'autism_post_features_tfidf_256.csv',\n",
    "        'lonely_post_features_tfidf_256.csv']\n",
    "\n",
    "\n",
    "pre_sample = 2700\n",
    "mid_sample = 1300\n",
    "sample = pre_sample if pre_pandemic else mid_sample\n",
    "files = pre_files if pre_pandemic else mid_files\n",
    "\n",
    "# can use these dates for generating the mid-pandemic model with the acute phase of the pandemic\n",
    "# to use, uncomment date lines below\n",
    "beg_date = datetime.datetime(2020, 3, 16)\n",
    "end_date = datetime.datetime(2020, 4, 20)\n",
    "\n",
    "health_anxiety_file = 'healthanxiety_pre_features_tfidf_256.csv' if pre_pandemic else 'healthanxiety_post_features_tfidf_256.csv'\n",
    "\n",
    "data = pd.read_csv(os.path.join(data_folder, health_anxiety_file))\n",
    "data = data.sample(sample)\n",
    "# data['date'] = pd.to_datetime(data['date'], format=\"%Y/%m/%d\")\n",
    "# data = data.loc[(data['date'] >= beg_date)]\n",
    "# data =  data.loc[(data['date'] < end_date)]\n",
    "data_text = data.loc[:,['post']]\n",
    "\n",
    "for file in files:\n",
    "    data = pd.read_csv(os.path.join(data_folder, file));\n",
    "    data = data.sample(sample)\n",
    "#     data['date'] = pd.to_datetime(data['date'], format=\"%Y/%m/%d\")\n",
    "#     data = data.loc[(data['date'] >= beg_date)]\n",
    "#     data =  data.loc[(data['date'] < end_date)]\n",
    "    data_text = pd.concat([data_text, data.loc[:,['post']]], axis=0, ignore_index=True)\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xo77BHhId6Iv"
   },
   "outputs": [],
   "source": [
    "# Look at shape of documents and a sample post\n",
    "print(documents.shape)\n",
    "# print(documents.iloc[20][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvlBjW3ad6I2"
   },
   "outputs": [],
   "source": [
    "# Methods for stemming and lemmatizing all posts\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTVfMIh5d6I7"
   },
   "outputs": [],
   "source": [
    "# Preprocess all documents to stem and lemmatize the words\n",
    "processed_docs = documents['post'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XOcdiNzE6eIX",
    "outputId": "cf39cd98-e328-4805-d1c1-bd6ae862737b"
   },
   "outputs": [],
   "source": [
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(processed_docs, min_count=20)\n",
    "for idx in range(len(processed_docs)):\n",
    "    for token in bigram[processed_docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            processed_docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "cV2f5vudd6JA",
    "outputId": "add50d07-c8c7-470f-da55-5813b7147225"
   },
   "outputs": [],
   "source": [
    "# Use processed documents to create a dictionary of unigrams and bigrams. Filter to include only top 100k.\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "dictionary.filter_extremes(no_below=0.001, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "ecBoDDVRd6JD",
    "outputId": "c235571d-8d01-4e17-a7c3-0ae567c830be"
   },
   "outputs": [],
   "source": [
    "# Transform all processed documents into bag of words format based on dictionary.\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDpUVs6h5YL5"
   },
   "source": [
    "## Create and Save LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "FTg2nZ4xd6JK",
    "outputId": "91e54f70-1d9f-4b29-d11b-4d8701863b59"
   },
   "outputs": [],
   "source": [
    "# Create LDA model and print out topics\n",
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus, num_topics=10, id2word=dictionary, passes=25, workers=3)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBRSKQNed6JR"
   },
   "outputs": [],
   "source": [
    "# Process a sample document to see the topics that are contained in that document\n",
    "print(processed_docs[5000])\n",
    "\n",
    "for index, score in sorted(lda_model[bow_corpus[3]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "A8_xTFkVtcVj",
    "outputId": "4ef164d1-2bcd-48f7-d00f-760ff3beac09"
   },
   "outputs": [],
   "source": [
    "lda_output_path = os.path.join(output_path, 'lda_models')\n",
    "model_name = \"model_pre_10\"\n",
    "model_path_name = datapath(os.path.join(lda_output_path + model_name))\n",
    "\n",
    "# Save model to disk.\n",
    "# lda_model.save(temp_file)\n",
    "\n",
    "# Load a pretrained model from disk.\n",
    "lda_model = gensim.models.LdaMulticore.load(model_path_name)\n",
    "dictionary = gensim.corpora.Dictionary.load(os.path.join(lda_output_path, \"{}.id2word\".format(model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeEhjoTXtn30"
   },
   "source": [
    "## Apply LDA Model to all data\n",
    "\n",
    "Use LDA model created above - apply to all data and create a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels of LDA topics, depending on the model you have chosen\n",
    "pre_topics = ['Autism/ADHD + \\nSchool/Work', 'Alcohol/Addiction', 'Sleep Issues', \n",
    "              'Alcohol/Eating Disorders', 'Social Interaction', 'Schizophrenia', \n",
    "              'Medical/Medication', 'Health Anxiety', 'Mental Health Help', 'Life']\n",
    "\n",
    "mid_topics = ['Health Anxiety', 'Autism/Social', 'ADHD/Diagnosis', \n",
    "                             'Work/School/Home', 'Eating Disorder', 'Alcohol/Addiction', \n",
    "                             'Family', 'Sleep Issues', 'Social/Life', 'Mental Health/PTSD']\n",
    "\n",
    "distribution_output_path = os.path.join(output_path, 'lda_distribution')\n",
    "\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4p0wkqqZt6dg"
   },
   "source": [
    "Create heatmap and run LDA model on all pre-pandemic mental health posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "colab_type": "code",
    "id": "GNvZtyvdtmtp",
    "outputId": "4bc30670-da0f-4b04-851e-3f3131fc2685"
   },
   "outputs": [],
   "source": [
    "files = ['suicidewatch_pre_features_tfidf_256.csv',\n",
    "        'depression_pre_features_tfidf_256.csv',\n",
    "        'ptsd_pre_features_tfidf_256.csv',\n",
    "        'anxiety_pre_features_tfidf_256.csv',\n",
    "        'socialanxiety_pre_features_tfidf_256.csv',\n",
    "        'healthanxiety_pre_features_tfidf_256.csv',\n",
    "        'bipolarreddit_pre_features_tfidf_256.csv',\n",
    "        'bpd_pre_features_tfidf_256.csv',\n",
    "        'schizophrenia_pre_features_tfidf_256.csv',\n",
    "        'EDAnonymous_pre_features_tfidf_256.csv',\n",
    "        'alcoholism_pre_features_tfidf_256.csv',\n",
    "        'addiction_pre_features_tfidf_256.csv',\n",
    "        'adhd_pre_features_tfidf_256.csv',\n",
    "         'autism_pre_features_tfidf_256.csv',\n",
    "         'lonely_pre_features_tfidf_256.csv',\n",
    "        'mentalhealth_pre_features_tfidf_256.csv'\n",
    "        ]\n",
    "\n",
    "pre_mental_df = pd.DataFrame(columns=range(num_topics))\n",
    "\n",
    "for file in files:\n",
    "  df = pd.read_csv(os.path.join(data_folder, file));\n",
    "  posts = df.post\n",
    "  posts = [dictionary.doc2bow(preprocess(post)) for post in posts]\n",
    "  scores = np.zeros(shape=num_topics)\n",
    "  for post in posts:\n",
    "    topics = lda_model[post]\n",
    "    for index, score in topics:\n",
    "      scores[index] += score\n",
    "  scores /= len(posts)\n",
    "  # Add a new row at index k with values provided in list\n",
    "  pre_mental_df.loc[os.path.splitext(file)[0]] = scores.tolist()\n",
    "\n",
    "pre_mental_df.to_csv(os.path.join(distribution_output_path, 'pre_mentalhealth_distribution.csv'))\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "ylabels = [file.split('_')[0] for file in files]\n",
    "sns.set(style='white', font_scale=1, palette=sns.color_palette(\"husl\",15))\n",
    "chart = sns.heatmap(pre_mental_df, vmin=0, vmax=0.75, annot=True, xticklabels=pre_topics, yticklabels=ylabels)\n",
    "chart.set_xticklabels(\n",
    "    chart.get_xticklabels(), \n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontsize=15\n",
    "\n",
    ")\n",
    "chart.set_yticklabels(\n",
    "    chart.get_yticklabels(),\n",
    "    fontsize=15\n",
    "\n",
    ")\n",
    "plt.title('Pre-Pandemic Mental Health LDA')\n",
    "plt.xlabel('Topics', fontsize=15)\n",
    "plt.ylabel('Subreddit', fontsize=15)\n",
    "plt.savefig(os.path.join(distribution_output_path, 'pre_mentalhealth.png'), \n",
    "            format='png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNAnQO4y-jIU"
   },
   "source": [
    "Create heatmap and run LDA model on all mid-pandemic mental health posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "colab_type": "code",
    "id": "5ph7nyCjy71W",
    "outputId": "dc54ed9e-5357-4aaa-9e3e-62761ce8da52"
   },
   "outputs": [],
   "source": [
    "files = ['suicidewatch_post_features_tfidf_256.csv',\n",
    "        'depression_post_features_tfidf_256.csv',\n",
    "        'ptsd_post_features_tfidf_256.csv',\n",
    "        'anxiety_post_features_tfidf_256.csv',\n",
    "        'socialanxiety_post_features_tfidf_256.csv',\n",
    "        'healthanxiety_post_features_tfidf_256.csv',\n",
    "        'bipolarreddit_post_features_tfidf_256.csv',\n",
    "        'bpd_post_features_tfidf_256.csv',\n",
    "        'schizophrenia_post_features_tfidf_256.csv',\n",
    "        'EDAnonymous_post_features_tfidf_256.csv',\n",
    "        'alcoholism_post_features_tfidf_256.csv',\n",
    "        'addiction_post_features_tfidf_256.csv',\n",
    "        'adhd_post_features_tfidf_256.csv',\n",
    "         'autism_post_features_tfidf_256.csv',\n",
    "         'lonely_post_features_tfidf_256.csv',\n",
    "        'mentalhealth_post_features_tfidf_256.csv'\n",
    "        ]\n",
    "\n",
    "mid_mental_df = pd.DataFrame(columns=range(num_topics))\n",
    "beg_date = datetime.datetime(2020, 3, 16)\n",
    "\n",
    "for file in files:\n",
    "  df = pd.read_csv(os.path.join(data_folder, file));\n",
    "  df['date'] = pd.to_datetime(df['date'], format=\"%Y/%m/%d\")\n",
    "  df_mid = df.loc[df['date'] >= beg_date]\n",
    "  posts = df_mid.post\n",
    "  posts = [dictionary.doc2bow(preprocess(post)) for post in posts]\n",
    "  scores = np.zeros(shape=num_topics)\n",
    "  for post in posts:\n",
    "    topics = lda_model[post]\n",
    "    for index, score in topics:\n",
    "      scores[index] += score\n",
    "  scores /= len(posts)\n",
    "  # Add a new row at index k with values provided in list\n",
    "  mid_mental_df.loc[os.path.splitext(file)[0]] = scores.tolist()\n",
    "\n",
    "mid_mental_df.to_csv(os.path.join(distribution_output_path, 'mid_mentalhealth_distribution.csv'))\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "ylabels = [file.split('_')[0] for file in files]\n",
    "sns.set(style='white', font_scale=1, palette=sns.color_palette(\"husl\",15))\n",
    "chart = sns.heatmap(mid_mental_df, vmin=0, vmax=0.75, annot=True, \n",
    "                    xticklabels=pre_topics, yticklabels=ylabels)\n",
    "chart.set_xticklabels(\n",
    "    chart.get_xticklabels(), \n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontsize=15\n",
    "\n",
    ")\n",
    "chart.set_yticklabels(\n",
    "    chart.get_yticklabels(),\n",
    "    fontsize=15\n",
    "\n",
    ")\n",
    "plt.title('Mid-Pandemic Mental Health LDA')\n",
    "plt.xlabel('Topics', fontsize=15)\n",
    "plt.ylabel('Subreddit', fontsize=15)\n",
    "plt.savefig(os.path.join(distribution_output_path, 'mid_mentalhealth.png'), \n",
    "            format='png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "96qAcFQa-sgT"
   },
   "source": [
    "Create heatmap and run LDA model on all pre-pandemic non-mental health posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "colab_type": "code",
    "id": "90kZGmX4-xj6",
    "outputId": "556860ce-3b7b-4da9-da04-9b7f61d1cbb2"
   },
   "outputs": [],
   "source": [
    "files = ['meditation_pre_features_tfidf_256.csv',\n",
    "          'personalfinance_pre_features_tfidf_256.csv',\n",
    "          'teaching_pre_features_tfidf_256.csv',\n",
    "          'relationships_pre_features_tfidf_256.csv',\n",
    "          'legaladvice_pre_features_tfidf_256.csv',\n",
    "          'fitness_pre_features_tfidf_256.csv',\n",
    "          'parenting_pre_features_tfidf_256.csv',\n",
    "          'divorce_pre_features_tfidf_256.csv',\n",
    "         'conspiracy_pre_features_tfidf_256.csv',\n",
    "         'guns_pre_features_tfidf_256.csv',\n",
    "         'jokes_pre_features_tfidf_256.csv']\n",
    "\n",
    "pre_nonmentalhealth_df = pd.DataFrame(columns=range(num_topics))\n",
    "\n",
    "for file in files:\n",
    "  df = pd.read_csv(os.path.join(data_folder, file));\n",
    "  posts = df.post\n",
    "  posts = [dictionary.doc2bow(preprocess(post)) for post in posts]\n",
    "  scores = np.zeros(shape=num_topics)\n",
    "  for post in posts:\n",
    "    topics = lda_model[post]\n",
    "    for index, score in topics:\n",
    "      scores[index] += score\n",
    "  scores /= len(posts)\n",
    "  # Add a new row at index k with values provided in list\n",
    "  pre_nonmentalhealth_df.loc[os.path.splitext(file)[0]] = scores.tolist()\n",
    "\n",
    "pre_nonmentalhealth_df.to_csv(os.path.join(distribution_output_path, 'pre_nonmentalhealth_distribution.csv'))\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "ylabels = [file.split('_')[0] for file in files]\n",
    "sns.set(style='white', font_scale=1, palette=sns.color_palette(\"husl\",15))\n",
    "chart = sns.heatmap(pre_nonmentalhealth_df, vmin=0, vmax=0.75, annot=True, \n",
    "                    xticklabels=pre_topics, yticklabels=ylabels)\n",
    "chart.set_xticklabels(\n",
    "    chart.get_xticklabels(), \n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontsize=15\n",
    "\n",
    ")\n",
    "chart.set_yticklabels(\n",
    "    chart.get_yticklabels(),\n",
    "    fontsize=15\n",
    "\n",
    ")\n",
    "plt.title('Pre-Pandemic Non-Mental Health LDA')\n",
    "plt.xlabel('Topics', fontsize=15)\n",
    "plt.ylabel('Subreddit', fontsize=15)\n",
    "plt.savefig(os.path.join(distribution_output_path, 'pre_nonmentalhealth.png'), \n",
    "            format='png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxLRyAwpPPmW"
   },
   "source": [
    "Create heatmap and run LDA model on all mid-pandemic control posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "colab_type": "code",
    "id": "F2dYOFxQ_kMD",
    "outputId": "2437b80a-b999-4227-cc75-6ff1c0761f89"
   },
   "outputs": [],
   "source": [
    "files = ['meditation_post_features_tfidf_256.csv',\n",
    "          'personalfinance_post_features_tfidf_256.csv',\n",
    "          'teaching_post_features_tfidf_256.csv',\n",
    "          'relationships_post_features_tfidf_256.csv',\n",
    "          'legaladvice_post_features_tfidf_256.csv',\n",
    "          'fitness_post_features_tfidf_256.csv',\n",
    "          'parenting_post_features_tfidf_256.csv',\n",
    "          'divorce_post_features_tfidf_256.csv',\n",
    "         'conspiracy_post_features_tfidf_256.csv',\n",
    "         'guns_post_features_tfidf_256.csv',\n",
    "         'jokes_post_features_tfidf_256.csv']\n",
    "\n",
    "mid_nonmentalhealth_df = pd.DataFrame(columns=range(num_topics))\n",
    "end_date = datetime.datetime(2020, 3, 16)\n",
    "\n",
    "for file in files:\n",
    "  df = pd.read_csv(os.path.join(data_folder, file));\n",
    "  df['date'] = pd.to_datetime(df['date'], format=\"%Y/%m/%d\")\n",
    "  df_mid = df.loc[df['date'] >= end_date]\n",
    "  posts = df_mid.post\n",
    "  posts = [dictionary.doc2bow(preprocess(post)) for post in posts]\n",
    "  scores = np.zeros(shape=num_topics)\n",
    "  for post in posts:\n",
    "    topics = lda_model[post]\n",
    "    for index, score in topics:\n",
    "      scores[index] += score\n",
    "  scores /= len(posts)\n",
    "  # Add a new row at index k with values provided in list\n",
    "  mid_nonmentalhealth_df.loc[os.path.splitext(file)[0]] = scores.tolist()\n",
    "\n",
    "mid_nonmentalhealth_df.to_csv(os.path.join(distribution_output_path, 'mid_nonmentalhealth_distribution.csv'))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "ylabels = [file.split('_')[0] for file in files]\n",
    "sns.set(style='white', font_scale=1, palette=sns.color_palette(\"husl\",15))\n",
    "chart = sns.heatmap(mid_nonmentalhealth_df, vmin=0, vmax=0.75, annot=True, \n",
    "                    xticklabels=pre_topics, yticklabels=ylabels)\n",
    "chart.set_xticklabels(\n",
    "    chart.get_xticklabels(), \n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontsize=15\n",
    "\n",
    ")\n",
    "chart.set_yticklabels(\n",
    "    chart.get_yticklabels(),\n",
    "    fontsize=15\n",
    "\n",
    ")\n",
    "plt.title('Mid-Pandemic Non-Mental Health LDA')\n",
    "plt.xlabel('Topics', fontsize=15)\n",
    "plt.ylabel('Subreddit', fontsize=15)\n",
    "plt.savefig(os.path.join(distribution_output_path, 'pre_nonmentalhealth.png'), \n",
    "            format='png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qp3gH2QIPuqj"
   },
   "source": [
    "Create heatmap from LDA of pre-pandemic posts on all COVID19_Support posts to determine distribution of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "colab_type": "code",
    "id": "8kelxLm9iCQP",
    "outputId": "25d419a6-6ff9-4e7d-fc7b-197683abd1a6"
   },
   "outputs": [],
   "source": [
    "files = ['COVID19_support_post_features_tfidf_256.csv']\n",
    "\n",
    "covid_topic_df = pd.DataFrame(columns=range(num_topics))\n",
    "end_date = datetime.datetime(2020, 3, 16)\n",
    "\n",
    "for file in files:\n",
    "  df = pd.read_csv(os.path.join(data_folder, file));\n",
    "  df['date'] = pd.to_datetime(df['date'], format=\"%Y/%m/%d\")\n",
    "  df_mid = df.loc[df['date'] >= end_date]\n",
    "  posts = df_mid.post\n",
    "  posts = [dictionary.doc2bow(preprocess(post)) for post in posts]\n",
    "  scores = np.zeros(shape=num_topics)\n",
    "  for post in posts:\n",
    "    topics = lda_model[post]\n",
    "    for index, score in topics:\n",
    "      scores[index] += score\n",
    "  scores /= len(posts)\n",
    "  # Add a new row at index k with values provided in list\n",
    "  covid_topic_df.loc[os.path.splitext(file)[0]] = scores.tolist()\n",
    "\n",
    "covid_topic_df.to_csv(os.path.join(distribution_output_path, 'covid_distribution_pre_model.csv'))\n",
    "\n",
    "covid_topic_df_transpose = covid_topic_df.transpose()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "xlabels = ['COVID19_support']\n",
    "sns.set(style='white', font_scale=1, palette=sns.color_palette(\"husl\",15))\n",
    "svm = sns.heatmap(covid_topic_df_transpose, vmin=0, vmax=0.6, annot=True, \n",
    "            cbar=False, xticklabels=pre_labels, yticklabels=ylabels, square=True)\n",
    "plt.ylabel('Topics')\n",
    "\n",
    "plt.savefig(os.path.join(distribution_output_path, 'covid19_pre_model.png'), \n",
    "            format='png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1WRM9XUP5K4"
   },
   "source": [
    "Create heatmap from LDA of mid-pandemic posts on all COVID19_Support posts to determine distribution of topics. Requires loading the appropriate model into lda_model and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['COVID19_support_post_features_tfidf_256.csv']\n",
    "\n",
    "covid_topic_df = pd.DataFrame(columns=range(num_topics))\n",
    "end_date = datetime.datetime(2020, 3, 16)\n",
    "\n",
    "for file in files:\n",
    "  df = pd.read_csv(os.path.join(data_folder, file));\n",
    "  df['date'] = pd.to_datetime(df['date'], format=\"%Y/%m/%d\")\n",
    "  df_mid = df.loc[df['date'] >= end_date]\n",
    "  posts = df_mid.post\n",
    "  posts = [dictionary.doc2bow(preprocess(post)) for post in posts]\n",
    "  scores = np.zeros(shape=num_topics)\n",
    "  for post in posts:\n",
    "    topics = lda_model[post]\n",
    "    for index, score in topics:\n",
    "      scores[index] += score\n",
    "  scores /= len(posts)\n",
    "  # Add a new row at index k with values provided in list\n",
    "  covid_topic_df.loc[os.path.splitext(file)[0]] = scores.tolist()\n",
    "\n",
    "covid_topic_df.to_csv(os.path.join(distribution_output_path, 'covid_distribution_mid_model.csv'))\n",
    "\n",
    "covid_topic_df_transpose = covid_topic_df.transpose()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "xlabels = ['COVID19_support']\n",
    "sns.set(style='white', font_scale=1, palette=sns.color_palette(\"husl\",15))\n",
    "svm = sns.heatmap(covid_topic_df_transpose, vmin=0, vmax=0.6, annot=True, \n",
    "            cbar=False, xticklabels=pre_labels, yticklabels=ylabels, square=True)\n",
    "plt.ylabel('Topics')\n",
    "\n",
    "plt.savefig(os.path.join(distribution_output_path, 'covid19_mid_model.png'), \n",
    "            format='png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_combined_df = pd.concat([pre_mental_df, pre_nonmentalhealth_df])\n",
    "pre_combined.to_csv(os.path.join(distribution_output_path, 'pre_distribution.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_combined_df = pd.concat([mid_mental_df, mid_nonmentalhealth_df])\n",
    "mid_combined_df.to_csv(os.path.join(distribution_output_path, 'mid_distribution.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "for i in range(0, 10):\n",
    "    print(scipy.stats.wilcoxon(pre_combined_df[i], mid_combined_df[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "reddit_lda_pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
